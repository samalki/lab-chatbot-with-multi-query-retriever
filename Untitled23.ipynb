{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Choose data you want to use as your knowledge base\n",
        "Choose any vector db\n",
        "Embed your documents and store them\n",
        "Write RAG (retrieval + generation)\n",
        "Ask it 5-10 questions. See when it works well and when it doesnt\n",
        "For the presentation:\n",
        "Showing ipynb is good enough!\n",
        "Explain what you did and the logic\n",
        "Explain the problems you encountered (any problems: technical or with response quality or whatever)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dqkZCwb4HHxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install packages\n"
      ],
      "metadata": {
        "id": "RqrZ0wnJMLO1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dq3Lg3H1HGU0",
        "outputId": "210d7b6d-f6ca-428b-d677-c0d4b2877dfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m746.6/746.6 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m895.2/895.2 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m433.9/433.9 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU jq lark langchain langchain-elasticsearch langchain_openai tiktoken wikipedia\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain langchain-community langchain-openai langchain-elasticsearch wikipedia tiktoken\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUpcaPEVIAsp",
        "outputId": "89bf3c38-146b-40fe-b026-2204fef70e93"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.14)\n",
            "Requirement already satisfied: langchain-elasticsearch in /usr/local/lib/python3.11/dist-packages (0.3.2)\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.54)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.31)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.75.0)\n",
            "Requirement already satisfied: elasticsearch<9.0.0,>=8.13.1 in /usr/local/lib/python3.11/dist-packages (from elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch) (8.18.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (4.13.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.19.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: elastic-transport<9,>=8.15.1 in /usr/local/lib/python3.11/dist-packages (from elasticsearch<9.0.0,>=8.13.1->elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch) (8.17.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from elasticsearch<9.0.0,>=8.13.1->elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from elasticsearch<9.0.0,>=8.13.1->elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch) (4.13.2)\n",
            "Requirement already satisfied: simsimd>=3 in /usr/local/lib/python3.11/dist-packages (from elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch) (6.2.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->elasticsearch<9.0.0,>=8.13.1->elasticsearch[vectorstore-mmr]<9.0.0,>=8.13.1->langchain-elasticsearch) (1.17.0)\n",
            "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.21 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import and define keys\n"
      ],
      "metadata": {
        "id": "WmKKItAzMYtU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing LangChain components (embeddings, retrievers, chains), and using getpass for secure input of API keys.**\n"
      ],
      "metadata": {
        "id": "Xx5TbMZcSy3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enables logging to monitor auto-generated multi-queries from LangChain.**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qEcZLw9yTF72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain_elasticsearch import ElasticsearchStore\n",
        "from langchain_openai.llms import OpenAI\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain.document_loaders import WikipediaLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
        "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain.schema import format_document\n",
        "from getpass import getpass\n",
        "import logging\n",
        "\n",
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
        "\n",
        "ELASTIC_CLOUD_ID = getpass(\"Elastic Cloud ID: \")\n",
        "ELASTIC_API_KEY = getpass(\"Elastic Api Key: \")\n",
        "OPENAI_API_KEY = getpass(\"OpenAI API Key: \")\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQrt0bRtHzt1",
        "outputId": "6c4b1f62-a563-4a5d-a0fc-7cad6f6c2a31"
      },
      "execution_count": 25,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Elastic Cloud ID: 路路路路路路路路路路\n",
            "Elastic Api Key: 路路路路路路路路路路\n",
            "OpenAI API Key: 路路路路路路路路路路\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splits documents into manageable chunks (800 characters with 400 overlap) using token-aware splitting.\n"
      ],
      "metadata": {
        "id": "65SNsneTTkyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = WikipediaLoader(query=[\"Artificial Intelligence\", \"Machine Learning\", \"Neural Networks\"], load_max_docs=3)\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=800, chunk_overlap=400)\n",
        "docs = text_splitter.split_documents(documents)\n"
      ],
      "metadata": {
        "id": "JICqOqQMJuxF"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creates a new index on Elasticsearch and stores the vectorized chunks from Wikipedia.\n"
      ],
      "metadata": {
        "id": "Ovki3iLzTxSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = ElasticsearchStore(\n",
        "    es_cloud_id=ELASTIC_CLOUD_ID,\n",
        "    es_api_key=ELASTIC_API_KEY,\n",
        "    index_name=\"wiki-ai\",\n",
        "    embedding=embeddings,\n",
        ")\n",
        "\n",
        "vectorstore = ElasticsearchStore.from_documents(\n",
        "    docs,\n",
        "    embeddings,\n",
        "    index_name=\"wiki-ai\",\n",
        "    es_cloud_id=ELASTIC_CLOUD_ID,\n",
        "    es_api_key=ELASTIC_API_KEY,\n",
        ")\n"
      ],
      "metadata": {
        "id": "B90bfS0wJ0J9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializes OpenAI LLM and builds a MultiQueryRetriever to generate diverse queries per question.\n",
        "\n"
      ],
      "metadata": {
        "id": "JQ07nK-1T-DQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
        "retriever = MultiQueryRetriever.from_llm(vectorstore.as_retriever(), llm)\n",
        "\n",
        "LLM_DOCUMENT_PROMPT = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "---\n",
        "SOURCE: Wikipedia\n",
        "{page_content}\n",
        "---\"\"\"\n",
        ")\n",
        "\n",
        "LLM_CONTEXT_PROMPT = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Be as verbose and educational as possible.\n",
        "\n",
        "    context: {context}\n",
        "    Question: \"{question}\"\n",
        "    Answer:\n",
        "    \"\"\"\n",
        ")\n",
        "#A helper function to combine retrieved documents into a single context block.\n",
        "\n",
        "\n",
        "def _combine_documents(docs, document_prompt=LLM_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"):\n",
        "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
        "    return document_separator.join(doc_strings)\n",
        "\n",
        "#Builds a chain that retrieves context, formats the prompt, and queries the LLM for answers.\n",
        "\n",
        "\n",
        "_context = RunnableParallel(\n",
        "    context=retriever | _combine_documents,\n",
        "    question=RunnablePassthrough(),\n",
        ")\n",
        "\n",
        "chain = _context | LLM_CONTEXT_PROMPT | llm\n"
      ],
      "metadata": {
        "id": "3-fgk59ELG3p"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sends a list of AI-related questions through the chain and prints out the LLM-generated answers.\n",
        "\n"
      ],
      "metadata": {
        "id": "f3z-Vl2QUfeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"What is artificial intelligence?\",\n",
        "    \"How does machine learning differ from deep learning?\",\n",
        "    \"What are neural networks and how do they work?\",\n",
        "    \"Who were pioneers in the field of AI?\",\n",
        "    \"What is the history of AI research?\",\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    print(f\"\\n Question: {q}\")\n",
        "    print(\" Answer:\")\n",
        "    print(chain.invoke(q))\n",
        "    print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-bXi4Y6LMg4",
        "outputId": "2360c391-09d7-4513-c4bc-f92960f4d373"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Question: What is artificial intelligence?\n",
            " Answer:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you provide information on artificial intelligence?', '2. How would you define artificial intelligence?', '3. What are the key concepts of artificial intelligence?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Artificial intelligence (AI) is a field of computer science that focuses on creating intelligent machines that can perform tasks that typically require human intelligence. This includes tasks such as visual perception, speech recognition, decision-making, and language translation. Artificial neural networks, which are mathematical models inspired by the structure and functions of biological neural networks, are often used in AI to approximate nonlinear functions and solve problems. AI has a wide range of applications, including predictive modeling, adaptive control, and solving problems in various fields such as healthcare, finance, and transportation.\n",
            "================================================================================\n",
            "\n",
            " Question: How does machine learning differ from deep learning?\n",
            " Answer:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. What are the key differences between machine learning and deep learning?', '2. In what ways does deep learning set itself apart from traditional machine learning techniques?', '3. Can you explain the distinctions between machine learning and deep learning?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Machine learning is a broad field that encompasses various techniques and algorithms for teaching computers to learn from data and make predictions or decisions without being explicitly programmed. Deep learning is a subset of machine learning that uses artificial neural networks with multiple layers to learn from data and make predictions or decisions. Deep learning is inspired by the structure and functions of biological neural networks and has shown significant improvements in performance compared to traditional machine learning techniques. Deep learning networks, such as convolutional neural networks, are particularly effective for tasks involving image and speech recognition, natural language processing, and other complex tasks. In contrast, traditional machine learning techniques often require hand-engineered features and may not be as effective for complex tasks.\n",
            "================================================================================\n",
            "\n",
            " Question: What are neural networks and how do they work?\n",
            " Answer:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you explain the concept of neural networks and their functioning?', '2. How do neural networks operate and what is their purpose?', '3. What is the working principle behind neural networks and their applications?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Neural networks are computational models inspired by the structure and functions of biological neural networks. They consist of interconnected units called neurons that send signals to one another. These neurons can be either biological cells or mathematical models. While individual neurons are simple, many of them together in a network can perform complex tasks. There are two main types of neural networks: biological neural networks found in brains and complex nervous systems, and artificial neural networks used in machine learning to approximate nonlinear functions.\n",
            "\n",
            "In a biological neural network, neurons are connected by synapses and send and receive electrochemical signals called action potentials. These signals can serve an excitatory or inhibitory role, and populations of interconnected neurons form neural circuits and large scale brain networks. The signals generated by neural networks in the brain eventually travel through the nervous system and across neuromuscular junctions to muscle cells, causing motion.\n",
            "\n",
            "In machine learning, artificial neural networks are implemented in software and consist of layers of interconnected neurons. The input layer receives information, which is then processed by one or more hidden layers before being outputted by the final layer. The strength of the signal at each connection is determined by a weight, which is adjusted during the learning process. Neural networks are trained by minimizing the difference between predicted and actual values in a dataset,\n",
            "================================================================================\n",
            "\n",
            " Question: Who were pioneers in the field of AI?\n",
            " Answer:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you provide a list of pioneers in the field of AI?', '2. Who are some notable figures in the history of AI?', '3. Which individuals played a significant role in the development of AI?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Some pioneers in the field of AI include Alexander Bain, William James, Donald Hebb, Warren McCulloch, and Walter Pitts.\n",
            "================================================================================\n",
            "\n",
            " Question: What is the history of AI research?\n",
            " Answer:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you provide information on the evolution of AI research?', '2. How has the field of AI research developed over time?', '3. What are the key milestones in the history of AI research?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The history of AI research can be traced back to the 19th century, when Alexander Bain and William James proposed the idea that human thought could be modeled by interactions among neurons in the brain. In the 1940s, Donald Hebb introduced the concept of Hebbian learning, which laid the foundation for neural networks to change and learn over time. In the 1950s, the invention of the perceptron by Warren McCulloch and Walter Pitts and its implementation in hardware by Frank Rosenblatt marked the beginning of using artificial neural networks for machine learning applications. This led to a shift away from modeling biological neural networks and towards developing artificial neural networks for solving problems in artificial intelligence. Today, neural networks are widely used in various fields, including predictive modeling, adaptive control, facial recognition, and handwriting recognition.\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}